<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="https://bigshuimu.oss-cn-nanjing.aliyuncs.com/personal/favicon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="https://bigshuimu.oss-cn-nanjing.aliyuncs.com/personal/favicon.png">
  <link rel="icon" type="image/png" sizes="16x16" href="https://bigshuimu.oss-cn-nanjing.aliyuncs.com/personal/favicon.png">
  <link rel="mask-icon" href="https://bigshuimu.oss-cn-nanjing.aliyuncs.com/personal/favicon.png" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"bigshuimu.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="RQ_VAE">
<meta property="og:type" content="article">
<meta property="og:title" content="RQ_VAE">
<meta property="og:url" content="https://bigshuimu.github.io/2025/03/13/RQ-VAE/index.html">
<meta property="og:site_name" content="QY的个人博客">
<meta property="og:description" content="RQ_VAE">
<meta property="og:locale">
<meta property="og:image" content="https://bigshuimu.github.io/2025/03/13/RQ-VAE/IMG-20250421094702507.png">
<meta property="og:image" content="https://bigshuimu.github.io/2025/03/13/RQ-VAE/2025-04-22-16-10-00.png">
<meta property="article:published_time" content="2025-03-13T02:51:52.000Z">
<meta property="article:modified_time" content="2025-04-22T08:10:05.712Z">
<meta property="article:author" content="QY">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://bigshuimu.github.io/2025/03/13/RQ-VAE/IMG-20250421094702507.png">

<link rel="canonical" href="https://bigshuimu.github.io/2025/03/13/RQ-VAE/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-Hans'
  };
</script>

  <title>RQ_VAE | QY的个人博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">QY的个人博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="https://bigshuimu.github.io/2025/03/13/RQ-VAE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://bigshuimu.oss-cn-nanjing.aliyuncs.com/personal/cat.jpg">
      <meta itemprop="name" content="QY">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="QY的个人博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          RQ_VAE
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-03-13 10:51:52" itemprop="dateCreated datePublished" datetime="2025-03-13T10:51:52+08:00">2025-03-13</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-04-22 16:10:05" itemprop="dateModified" datetime="2025-04-22T16:10:05+08:00">2025-04-22</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87/" itemprop="url" rel="index"><span itemprop="name">论文</span></a>
                </span>
            </span>

          
            <div class="post-description">RQ_VAE</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p><img src="IMG-20250421094702507.png" alt><br><img src="2025-04-22-16-10-00.png" alt></p>
<h1 id="Residual-Quantization"><a href="#Residual-Quantization" class="headerlink" title="Residual Quantization"></a>Residual Quantization</h1><h4 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a><strong>核心思想</strong></h4><p>残差量化（RQ）通过递归细化策略，将一个向量 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="1.156ex" height="1.005ex" role="img" focusable="false" viewbox="0 -444 511 444"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D433" d="M48 262Q48 264 54 349T60 436V444H252Q289 444 336 444T394 445Q441 445 450 441T459 418Q459 406 458 404Q456 399 327 229T194 55H237Q260 56 268 56T297 58T325 65T348 77T370 98T384 128T395 170Q400 197 400 216Q400 217 431 217H462V211Q461 208 453 108T444 6V0H245Q46 0 43 2Q32 7 32 28V33Q32 41 40 52T84 112Q129 170 164 217L298 393H256Q189 392 165 380Q124 360 115 303Q110 280 110 256Q110 254 79 254H48V262Z"/></g></g></g></g></svg></mjx-container>分解为多个有序的离散代码序列 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="11.658ex" height="2.262ex" role="img" focusable="false" viewbox="0 -750 5153 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="msub" transform="translate(389,0)"><g data-mml-node="mi"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"/></g><g data-mml-node="mn" transform="translate(554,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g></g><g data-mml-node="mo" transform="translate(1346.6,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="mo" transform="translate(1791.2,0)"><path data-c="22EF" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250ZM525 250Q525 274 542 292T585 310Q609 310 627 294T646 251Q646 226 629 208T586 190T543 207T525 250ZM972 250Q972 274 989 292T1032 310Q1056 310 1074 294T1093 251Q1093 226 1076 208T1033 190T990 207T972 250Z"/></g><g data-mml-node="mo" transform="translate(3129.9,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="msub" transform="translate(3574.6,0)"><g data-mml-node="mi"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"/></g><g data-mml-node="mi" transform="translate(554,-150) scale(0.707)"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"/></g></g><g data-mml-node="mo" transform="translate(4764,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g></svg></mjx-container>，从而以“由粗到细”的方式逼近原始向量。相比传统向量量化（VQ），RQ在相同代码本大小下能实现更精确的表示。 </p>
<hr>
<h4 id="具体流程"><a href="#具体流程" class="headerlink" title="具体流程"></a><strong>具体流程</strong></h4><ol>
<li><p><strong>初始化</strong>：<br>初始残差 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.375ex;" xmlns="http://www.w3.org/2000/svg" width="6.233ex" height="1.694ex" role="img" focusable="false" viewbox="0 -583 2755.1 748.6"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D42B" d="M405 293T374 293T324 312T305 361Q305 378 312 394Q315 397 315 399Q305 399 294 394T266 375T238 329T222 249Q221 241 221 149V62H308V0H298Q280 3 161 3Q47 3 38 0H29V62H98V210V303Q98 353 96 363T83 376Q69 380 42 380H29V442H32L118 446Q204 450 205 450H210V414L211 378Q247 449 315 449H321Q384 449 413 422T442 360Q442 332 424 313Z"/></g></g><g data-mml-node="mn" transform="translate(507,-150) scale(0.707)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"/></g></g><g data-mml-node="mo" transform="translate(1188.3,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(2244.1,0)"><g data-mml-node="mi"><path data-c="1D433" d="M48 262Q48 264 54 349T60 436V444H252Q289 444 336 444T394 445Q441 445 450 441T459 418Q459 406 458 404Q456 399 327 229T194 55H237Q260 56 268 56T297 58T325 65T348 77T370 98T384 128T395 170Q400 197 400 216Q400 217 431 217H462V211Q461 208 453 108T444 6V0H245Q46 0 43 2Q32 7 32 28V33Q32 41 40 52T84 112Q129 170 164 217L298 393H256Q189 392 165 380Q124 360 115 303Q110 280 110 256Q110 254 79 254H48V262Z"/></g></g></g></g></svg></mjx-container>。</p>
</li>
<li><p><strong>递归量化</strong>：<br>对于每个深度 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="12.239ex" height="2.009ex" role="img" focusable="false" viewbox="0 -694 5409.6 888"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g><g data-mml-node="mo" transform="translate(797.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mn" transform="translate(1853.6,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g><g data-mml-node="mo" transform="translate(2353.6,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="mo" transform="translate(2798.2,0)"><path data-c="22EF" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250ZM525 250Q525 274 542 292T585 310Q609 310 627 294T646 251Q646 226 629 208T586 190T543 207T525 250ZM972 250Q972 274 989 292T1032 310Q1056 310 1074 294T1093 251Q1093 226 1076 208T1033 190T990 207T972 250Z"/></g><g data-mml-node="mo" transform="translate(4136.9,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="mi" transform="translate(4581.6,0)"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"/></g></g></g></svg></mjx-container>：</p>
<ul>
<li><strong>选择代码</strong>：<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="15.159ex" height="2.262ex" role="img" focusable="false" viewbox="0 -750 6700.3 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"/></g><g data-mml-node="mi" transform="translate(554,-150) scale(0.707)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g></g><g data-mml-node="mo" transform="translate(1249.5,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(2305.3,0)"><g data-mml-node="mi"><path data-c="51" d="M114 286Q114 358 151 433T249 569T392 667T558 705Q653 705 713 641T774 460Q774 389 750 322T687 206T600 114T504 46T412 4L399 -2Q542 -62 636 -62Q660 -62 670 -54T686 -27T700 0Q734 34 770 34Q787 34 787 23Q787 -18 720 -74T563 -131Q485 -131 350 -83T145 -34Q127 -34 127 -22Q127 -12 144 5T190 31L200 34L237 35Q386 38 467 79Q550 120 612 210T675 416Q675 510 625 573T484 636Q410 636 346 587T248 469T214 333Q214 306 221 281T243 229T288 188T360 172Q403 172 441 188T490 205Q510 205 510 192Q505 162 432 132T287 102Q206 102 160 155T114 286Z"/></g></g><g data-mml-node="mo" transform="translate(3122.3,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="msub" transform="translate(3511.3,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D42B" d="M405 293T374 293T324 312T305 361Q305 378 312 394Q315 397 315 399Q305 399 294 394T266 375T238 329T222 249Q221 241 221 149V62H308V0H298Q280 3 161 3Q47 3 38 0H29V62H98V210V303Q98 353 96 363T83 376Q69 380 42 380H29V442H32L118 446Q204 450 205 450H210V414L211 378Q247 449 315 449H321Q384 449 413 422T442 360Q442 332 424 313Z"/></g></g><g data-mml-node="TeXAtom" transform="translate(507,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g><g data-mml-node="mo" transform="translate(520,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"/></g><g data-mml-node="mn" transform="translate(1298,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g></g></g><g data-mml-node="mo" transform="translate(5339.6,0)"><path data-c="3B" d="M78 370Q78 394 95 412T138 430Q162 430 180 414T199 371Q199 346 182 328T139 310T96 327T78 370ZM78 60Q78 85 94 103T137 121Q202 121 202 8Q202 -44 183 -94T144 -169T118 -194Q115 -194 106 -186T95 -174Q94 -171 107 -155T137 -107T160 -38Q161 -32 162 -22T165 -4T165 4Q165 5 161 4T142 0Q110 0 94 18T78 60Z"/></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(5784.3,0)"><g data-mml-node="mi"><path data-c="43" d="M201 -25Q167 -25 136 -14T75 23T29 94T12 202Q12 290 50 394T161 574Q227 642 303 673T433 704Q435 705 457 705Q533 701 533 640Q533 606 507 548T464 474Q431 444 396 444Q381 444 381 453Q381 459 388 473T407 513T428 563Q433 580 433 594Q433 636 381 636Q314 636 260 594T175 489T128 363T112 247Q112 157 153 101T273 44Q347 44 398 121Q413 144 437 157T481 171Q496 171 496 160Q496 150 476 123Q426 56 350 16T201 -25Z"/></g></g><g data-mml-node="mo" transform="translate(6311.3,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g></svg></mjx-container>，即从共享代码本 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.057ex;" xmlns="http://www.w3.org/2000/svg" width="1.192ex" height="1.652ex" role="img" focusable="false" viewbox="0 -705 527 730"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="43" d="M201 -25Q167 -25 136 -14T75 23T29 94T12 202Q12 290 50 394T161 574Q227 642 303 673T433 704Q435 705 457 705Q533 701 533 640Q533 606 507 548T464 474Q431 444 396 444Q381 444 381 453Q381 459 388 473T407 513T428 563Q433 580 433 594Q433 636 381 636Q314 636 260 594T175 489T128 363T112 247Q112 157 153 101T273 44Q347 44 398 121Q413 144 437 157T481 171Q496 171 496 160Q496 150 476 123Q426 56 350 16T201 -25Z"/></g></g></g></g></svg></mjx-container> 中找到与当前残差 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.471ex;" xmlns="http://www.w3.org/2000/svg" width="4.137ex" height="1.489ex" role="img" focusable="false" viewbox="0 -450 1828.4 658"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D42B" d="M405 293T374 293T324 312T305 361Q305 378 312 394Q315 397 315 399Q305 399 294 394T266 375T238 329T222 249Q221 241 221 149V62H308V0H298Q280 3 161 3Q47 3 38 0H29V62H98V210V303Q98 353 96 363T83 376Q69 380 42 380H29V442H32L118 446Q204 450 205 450H210V414L211 378Q247 449 315 449H321Q384 449 413 422T442 360Q442 332 424 313Z"/></g></g><g data-mml-node="TeXAtom" transform="translate(507,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g><g data-mml-node="mo" transform="translate(520,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"/></g><g data-mml-node="mn" transform="translate(1298,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g></g></g></g></g></svg></mjx-container> 最接近的代码。</li>
<li><strong>更新残差</strong>：<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="17.162ex" height="2.262ex" role="img" focusable="false" viewbox="0 -750 7585.8 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D42B" d="M405 293T374 293T324 312T305 361Q305 378 312 394Q315 397 315 399Q305 399 294 394T266 375T238 329T222 249Q221 241 221 149V62H308V0H298Q280 3 161 3Q47 3 38 0H29V62H98V210V303Q98 353 96 363T83 376Q69 380 42 380H29V442H32L118 446Q204 450 205 450H210V414L211 378Q247 449 315 449H321Q384 449 413 422T442 360Q442 332 424 313Z"/></g></g><g data-mml-node="mi" transform="translate(507,-150) scale(0.707)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g></g><g data-mml-node="mo" transform="translate(1202.5,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="msub" transform="translate(2258.3,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D42B" d="M405 293T374 293T324 312T305 361Q305 378 312 394Q315 397 315 399Q305 399 294 394T266 375T238 329T222 249Q221 241 221 149V62H308V0H298Q280 3 161 3Q47 3 38 0H29V62H98V210V303Q98 353 96 363T83 376Q69 380 42 380H29V442H32L118 446Q204 450 205 450H210V414L211 378Q247 449 315 449H321Q384 449 413 422T442 360Q442 332 424 313Z"/></g></g><g data-mml-node="TeXAtom" transform="translate(507,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g><g data-mml-node="mo" transform="translate(520,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"/></g><g data-mml-node="mn" transform="translate(1298,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g></g></g><g data-mml-node="mo" transform="translate(4308.9,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"/></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(5309.1,0)"><g data-mml-node="mi"><path data-c="1D41E" d="M32 225Q32 332 102 392T272 452H283Q382 452 436 401Q494 343 494 243Q494 226 486 222T440 217Q431 217 394 217T327 218H175V209Q175 177 179 154T196 107T236 69T306 50Q312 49 323 49Q376 49 410 85Q421 99 427 111T434 127T442 133T463 135H468Q494 135 494 117Q494 110 489 97T468 66T431 32T373 5T292 -6Q181 -6 107 55T32 225ZM383 276Q377 346 348 374T280 402Q253 402 230 390T195 357Q179 331 176 279V266H383V276Z"/></g></g><g data-mml-node="mo" transform="translate(5836.1,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="msub" transform="translate(6225.1,0)"><g data-mml-node="mi"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"/></g><g data-mml-node="mi" transform="translate(554,-150) scale(0.707)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g></g><g data-mml-node="mo" transform="translate(7196.8,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g></svg></mjx-container>，其中 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="5.151ex" height="2.262ex" role="img" focusable="false" viewbox="0 -750 2276.7 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D41E" d="M32 225Q32 332 102 392T272 452H283Q382 452 436 401Q494 343 494 243Q494 226 486 222T440 217Q431 217 394 217T327 218H175V209Q175 177 179 154T196 107T236 69T306 50Q312 49 323 49Q376 49 410 85Q421 99 427 111T434 127T442 133T463 135H468Q494 135 494 117Q494 110 489 97T468 66T431 32T373 5T292 -6Q181 -6 107 55T32 225ZM383 276Q377 346 348 374T280 402Q253 402 230 390T195 357Q179 331 176 279V266H383V276Z"/></g></g><g data-mml-node="mo" transform="translate(527,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="msub" transform="translate(916,0)"><g data-mml-node="mi"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"/></g><g data-mml-node="mi" transform="translate(554,-150) scale(0.707)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g></g><g data-mml-node="mo" transform="translate(1887.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g></svg></mjx-container> 是代码本中对应的嵌入向量。</li>
</ul>
</li>
<li><p><strong>部分和与最终量化</strong>：</p>
<ul>
<li><strong>部分和</strong>：<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.777ex;" xmlns="http://www.w3.org/2000/svg" width="16.859ex" height="2.966ex" role="img" focusable="false" viewbox="0 -967.8 7451.6 1311.1"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D433" d="M48 262Q48 264 54 349T60 436V444H252Q289 444 336 444T394 445Q441 445 450 441T459 418Q459 406 458 404Q456 399 327 229T194 55H237Q260 56 268 56T297 58T325 65T348 77T370 98T384 128T395 170Q400 197 400 216Q400 217 431 217H462V211Q461 208 453 108T444 6V0H245Q46 0 43 2Q32 7 32 28V33Q32 41 40 52T84 112Q129 170 164 217L298 393H256Q189 392 165 380Q124 360 115 303Q110 280 110 256Q110 254 79 254H48V262Z"/></g></g><g data-mml-node="mo" transform="translate(255.5,18) translate(-250 0)"><path data-c="5E" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z"/></g></g></g><g data-mml-node="TeXAtom" transform="translate(544,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g><g data-mml-node="mo" transform="translate(909,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g><g data-mml-node="mo" transform="translate(1789.6,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="munderover" transform="translate(2845.4,0)"><g data-mml-node="mo"><path data-c="2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"/></g><g data-mml-node="mi" transform="translate(1089,477.1) scale(0.707)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g><g data-mml-node="TeXAtom" transform="translate(1089,-285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mo" transform="translate(345,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mn" transform="translate(1123,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g></g></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(5298.7,0)"><g data-mml-node="mi"><path data-c="1D41E" d="M32 225Q32 332 102 392T272 452H283Q382 452 436 401Q494 343 494 243Q494 226 486 222T440 217Q431 217 394 217T327 218H175V209Q175 177 179 154T196 107T236 69T306 50Q312 49 323 49Q376 49 410 85Q421 99 427 111T434 127T442 133T463 135H468Q494 135 494 117Q494 110 489 97T468 66T431 32T373 5T292 -6Q181 -6 107 55T32 225ZM383 276Q377 346 348 374T280 402Q253 402 230 390T195 357Q179 331 176 279V266H383V276Z"/></g></g><g data-mml-node="mo" transform="translate(5825.7,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="msub" transform="translate(6214.7,0)"><g data-mml-node="mi"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"/></g><g data-mml-node="mi" transform="translate(554,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g></g><g data-mml-node="mo" transform="translate(7062.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g></svg></mjx-container>，表示前 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex;" xmlns="http://www.w3.org/2000/svg" width="1.176ex" height="1.593ex" role="img" focusable="false" viewbox="0 -694 520 704"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g></g></g></svg></mjx-container> 个代码嵌入的累加。</li>
<li><strong>最终量化结果</strong>：<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="8.086ex" height="2.207ex" role="img" focusable="false" viewbox="0 -893.3 3574.2 975.3"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D433" d="M48 262Q48 264 54 349T60 436V444H252Q289 444 336 444T394 445Q441 445 450 441T459 418Q459 406 458 404Q456 399 327 229T194 55H237Q260 56 268 56T297 58T325 65T348 77T370 98T384 128T395 170Q400 197 400 216Q400 217 431 217H462V211Q461 208 453 108T444 6V0H245Q46 0 43 2Q32 7 32 28V33Q32 41 40 52T84 112Q129 170 164 217L298 393H256Q189 392 165 380Q124 360 115 303Q110 280 110 256Q110 254 79 254H48V262Z"/></g></g><g data-mml-node="mo" transform="translate(255.5,18) translate(-250 0)"><path data-c="5E" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z"/></g></g></g><g data-mml-node="mo" transform="translate(788.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="msup" transform="translate(1844.6,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D433" d="M48 262Q48 264 54 349T60 436V444H252Q289 444 336 444T394 445Q441 445 450 441T459 418Q459 406 458 404Q456 399 327 229T194 55H237Q260 56 268 56T297 58T325 65T348 77T370 98T384 128T395 170Q400 197 400 216Q400 217 431 217H462V211Q461 208 453 108T444 6V0H245Q46 0 43 2Q32 7 32 28V33Q32 41 40 52T84 112Q129 170 164 217L298 393H256Q189 392 165 380Q124 360 115 303Q110 280 110 256Q110 254 79 254H48V262Z"/></g></g><g data-mml-node="mo" transform="translate(255.5,18) translate(-250 0)"><path data-c="5E" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z"/></g></g></g><g data-mml-node="TeXAtom" transform="translate(544,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"/></g><g data-mml-node="mo" transform="translate(1217,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g></g></g></svg></mjx-container>，即所有 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="1.873ex" height="1.545ex" role="img" focusable="false" viewbox="0 -683 828 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"/></g></g></g></svg></mjx-container> 个代码嵌入的总和。</li>
</ul>
</li>
</ol>
<hr>
<h4 id="共享代码本的优势"><a href="#共享代码本的优势" class="headerlink" title="共享代码本的优势"></a><strong>共享代码本的优势</strong></h4><ol>
<li><p><strong>简化超参数</strong>：<br>无需为每个深度单独设计代码本，仅需确定总代码本大小 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="2.011ex" height="1.545ex" role="img" focusable="false" viewbox="0 -683 889 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"/></g></g></g></svg></mjx-container>，大幅减少调参复杂度。</p>
</li>
<li><p><strong>提升代码利用率</strong>：<br>所有深度的量化共享同一代码本，允许代码在不同深度重复使用，最大化其效用。</p>
</li>
</ol>
<hr>
<h4 id="与VQ的对比"><a href="#与VQ的对比" class="headerlink" title="与VQ的对比"></a><strong>与VQ的对比</strong></h4><div class="table-container">
<table>
<thead>
<tr>
<th><strong>特性</strong></th>
<th><strong>VQ</strong></th>
<th><strong>RQ（深度 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="1.873ex" height="1.545ex" role="img" focusable="false" viewbox="0 -683 828 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"/></g></g></g></svg></mjx-container>）</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>分区能力</strong></td>
<td><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="2.011ex" height="1.545ex" role="img" focusable="false" viewbox="0 -683 889 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"/></g></g></g></svg></mjx-container> 个簇</td>
<td><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="3.641ex" height="1.914ex" role="img" focusable="false" viewbox="0 -846 1609.5 846"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"/></g><g data-mml-node="mi" transform="translate(974,363) scale(0.707)"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"/></g></g></g></g></svg></mjx-container> 个簇（等效于VQ的 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="3.641ex" height="1.914ex" role="img" focusable="false" viewbox="0 -846 1609.5 846"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"/></g><g data-mml-node="mi" transform="translate(974,363) scale(0.707)"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"/></g></g></g></g></svg></mjx-container> 代码本）</td>
</tr>
<tr>
<td><strong>量化精度</strong></td>
<td>单次粗粒度逼近</td>
<td>递归细化，逐步逼近（粗到细）</td>
</tr>
<tr>
<td><strong>代码本复杂度</strong></td>
<td>需指数级增大代码本（<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="3.641ex" height="1.914ex" role="img" focusable="false" viewbox="0 -846 1609.5 846"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"/></g><g data-mml-node="mi" transform="translate(974,363) scale(0.707)"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"/></g></g></g></g></svg></mjx-container>）</td>
<td>仅需线性增加深度 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="1.873ex" height="1.545ex" role="img" focusable="false" viewbox="0 -683 828 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"/></g></g></g></svg></mjx-container>，代码本大小保持 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="2.011ex" height="1.545ex" role="img" focusable="false" viewbox="0 -683 889 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"/></g></g></g></svg></mjx-container></td>
</tr>
</tbody>
</table>
</div>
<hr>
<h4 id="关键优势"><a href="#关键优势" class="headerlink" title="关键优势"></a><strong>关键优势</strong></h4><ol>
<li><strong>高效压缩</strong>：通过深度 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="1.873ex" height="1.545ex" role="img" focusable="false" viewbox="0 -683 828 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"/></g></g></g></svg></mjx-container> 的递归，以线性复杂度实现指数级的分区能力。</li>
<li><strong>灵活性</strong>：可根据需求调整深度 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="1.873ex" height="1.545ex" role="img" focusable="false" viewbox="0 -683 828 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"/></g></g></g></svg></mjx-container>，平衡精度与计算成本。</li>
<li><strong>训练稳定性</strong>：共享代码本避免了代码本崩溃（Codebook Collapse）问题。</li>
</ol>
<hr>
<h4 id="应用意义"><a href="#应用意义" class="headerlink" title="应用意义"></a><strong>应用意义</strong></h4><p>RQ提供了一种在有限代码本资源下提升量化精度的有效方法，尤其适用于需要高保真重建的任务（如图像生成、语音编码），同时为自回归模型（如PixelCNN）的高效训练奠定基础。</p>
<h1 id="RQ-VAE代码记录"><a href="#RQ-VAE代码记录" class="headerlink" title="RQ_VAE代码记录"></a>RQ_VAE代码记录</h1><h2 id="outputs-model-xs"><a href="#outputs-model-xs" class="headerlink" title="outputs = model(xs)"></a><code>outputs = model(xs)</code></h2><h3 id="输入输出"><a href="#输入输出" class="headerlink" title="输入输出"></a>输入输出</h3><p>输入：一批次归一化后的图像，shape为<code>{Tensor: (8, 3, 256, 256)}</code>，<br>输出：outputs <code>{tuple: 3}</code></p>
<ul>
<li><code>outputs[0]</code>: 重建图像，shape为<code>{Tensor: (8, 3, 256, 256)}</code></li>
<li><code>outputs[1]</code>: quant_loss <code>tensor(0.3732, device='cuda:0', grad_fn=&lt;MeanBackward0&gt;)</code></li>
<li><code>outputs[2]</code>:  code <code>torch.Size([8, 8, 8, 4])</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, xs</span>):</span><br><span class="line">	z_e = self.encode(xs)</span><br><span class="line">	z_q, quant_loss, code = self.quantizer(z_e)</span><br><span class="line">	out = self.decode(z_q)</span><br><span class="line">	<span class="keyword">return</span> out, quant_loss, code</span><br></pre></td></tr></table></figure>
<p>z_e是encode之后的结果，<code>{Tensor: (8, 8, 8, 256)}</code><br>quantizer</p>
<ul>
<li><code>z_q</code>：</li>
<li><code>quant_loss</code>：一个损失值</li>
<li><code>code</code>:<code>{Tensor: (8, 8, 8, 256)}</code> 值为码本索引<h2 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h2><h3 id="输入输出-1"><a href="#输入输出-1" class="headerlink" title="输入输出"></a>输入输出</h3>输入：一批次归一化后的图像</li>
</ul>
<h3 id="残差块"><a href="#残差块" class="headerlink" title="残差块"></a>残差块</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># downsampling</span></span><br><span class="line">hs = [self.conv_in(x)]</span><br><span class="line"><span class="keyword">for</span> i_level <span class="keyword">in</span> <span class="built_in">range</span>(self.num_resolutions): <span class="comment"># self.num_resolutions = 6</span></span><br><span class="line">	<span class="keyword">for</span> i_block <span class="keyword">in</span> <span class="built_in">range</span>(self.num_res_blocks): <span class="comment"># self.num_res_blocks = 2</span></span><br><span class="line">        h = self.down[i_level].block[i_block](hs[-<span class="number">1</span>], temb)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(self.down[i_level].attn) &gt; <span class="number">0</span>:</span><br><span class="line">            h = self.down[i_level].attn[i_block](h)</span><br><span class="line">        hs.append(h)</span><br><span class="line">	<span class="keyword">if</span> i_level != self.num_resolutions-<span class="number">1</span>:</span><br><span class="line">        hs.append(self.down[i_level].downsample(hs[-<span class="number">1</span>]))</span><br></pre></td></tr></table></figure>
<ol>
<li>先做一次卷积得到hs, <code>hs[0].shape = torch.Size([8, 128, 256, 256])</code></li>
</ol>
<div class="table-container">
<table>
<thead>
<tr>
<th>i_level</th>
<th>i_block</th>
<th><code>h = self.down[i_level].block[i_block](hs[-1], temb)</code></th>
<th><code>hs[-1]</code></th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0</td>
<td><code>{Tensor: (8, 128, 256, 256)}</code></td>
<td></td>
</tr>
<tr>
<td>0</td>
<td>1</td>
<td><code>{Tensor: (8, 128, 256, 256)}</code></td>
<td></td>
</tr>
<tr>
<td>1</td>
<td>0</td>
<td><code>{Tensor: (8, 128, 128, 128)}</code></td>
<td></td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td><code>{Tensor: (8, 128, 128, 128)}</code></td>
<td></td>
</tr>
<tr>
<td>2</td>
<td>0</td>
<td><code>{Tensor: (8, 256, 64, 64)}</code></td>
<td><code>{Tensor: (8, 128, 64, 64)}</code></td>
</tr>
<tr>
<td>2</td>
<td>1</td>
<td><code>{Tensor: (8, 256, 64, 64)}</code></td>
<td></td>
</tr>
<tr>
<td>3</td>
<td>0</td>
<td><code>{Tensor: (8, 256, 32, 32)}</code></td>
<td></td>
</tr>
<tr>
<td>3</td>
<td>1</td>
<td><code>{Tensor: (8, 256, 32, 32)}</code></td>
<td></td>
</tr>
<tr>
<td>4</td>
<td>0</td>
<td><code>{Tensor: (8, 512, 16, 16)}</code></td>
<td><code>{Tensor: (8, 256, 16, 16)}</code></td>
</tr>
<tr>
<td>4</td>
<td>1</td>
<td><code>{Tensor: (8, 512, 16, 16)}</code></td>
<td></td>
</tr>
<tr>
<td>5</td>
<td>0</td>
<td><code>{Tensor: (8, 512, 8, 8)}</code></td>
<td></td>
</tr>
<tr>
<td>5</td>
<td>1</td>
<td><code>{Tensor: (8, 512, 8, 8)}</code></td>
</tr>
</tbody>
</table>
</div>
<h3 id="第一次通过残差块"><a href="#第一次通过残差块" class="headerlink" title="第一次通过残差块"></a>第一次通过残差块</h3><p><code>h = self.down[i_level].block[i_block](hs[-1], temb) hs[-1].shape=torch.Size([8, 128, 256, 256]) temb=None</code><br><code>i_block=0, i_block=0</code></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>代码</th>
<th>输出</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>h = self.norm1(h)</code></td>
<td><code>torch.Size([8, 128, 256, 256])</code></td>
</tr>
<tr>
<td><code>h = nonlinearity(h)</code></td>
<td><code>torch.Size([8, 128, 256, 256])</code></td>
</tr>
<tr>
<td><code>h = self.conv1(h)</code></td>
<td><code>torch.Size([8, 128, 256, 256])</code></td>
</tr>
<tr>
<td><code>h = self.norm2(h)</code></td>
<td><code>torch.Size([8, 128, 256, 256])</code></td>
</tr>
<tr>
<td><code>h = nonlinearity(h)</code></td>
<td><code>torch.Size([8, 128, 256, 256])</code></td>
</tr>
<tr>
<td><code>h = self.dropout(h)</code></td>
<td><code>torch.Size([8, 128, 256, 256])</code></td>
</tr>
<tr>
<td><code>h = self.conv2(h)</code></td>
<td><code>torch.Size([8, 128, 256, 256])</code></td>
</tr>
<tr>
<td><code>return x+h</code></td>
<td><code>torch.Size([8, 128, 256, 256])</code></td>
</tr>
</tbody>
</table>
</div>
<p><code>hs.append(h)</code> 最后结果追加到hs中</p>
<hr>
<p><code>h = self.down[i_level].block[i_block](hs[-1], temb) hs[-1].shape=torch.Size([8, 128, 256, 256]) temb=None</code><br><code>i_block=0, i_block=1</code> <strong>注意</strong> i_block从0变成了1</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>代码</th>
<th>输出</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>h = self.norm1(h)</code></td>
<td><code>torch.Size([8, 128, 256, 256])</code></td>
</tr>
<tr>
<td><code>h = nonlinearity(h)</code></td>
<td><code>torch.Size([8, 128, 256, 256])</code></td>
</tr>
<tr>
<td><code>h = self.conv1(h)</code></td>
<td><code>torch.Size([8, 128, 256, 256])</code></td>
</tr>
<tr>
<td><code>h = self.norm2(h)</code></td>
<td><code>torch.Size([8, 128, 256, 256])</code></td>
</tr>
<tr>
<td><code>h = nonlinearity(h)</code></td>
<td><code>torch.Size([8, 128, 256, 256])</code></td>
</tr>
<tr>
<td><code>h = self.dropout(h)</code></td>
<td><code>torch.Size([8, 128, 256, 256])</code></td>
</tr>
<tr>
<td><code>h = self.conv2(h)</code></td>
<td><code>torch.Size([8, 128, 256, 256])</code></td>
</tr>
<tr>
<td><code>return x+h</code></td>
<td><code>torch.Size([8, 128, 256, 256])</code></td>
</tr>
</tbody>
</table>
</div>
<p><code>hs.append(h)</code> 最后结果追加到hs中</p>
<hr>
<p><code>i_level=0</code><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> i_level != self.num_resolutions-<span class="number">1</span>:</span><br><span class="line">    hs.append(self.down[i_level].downsample(hs[-<span class="number">1</span>]))</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">    <span class="keyword">if</span> self.with_conv: <span class="comment"># True</span></span><br><span class="line">        pad = (<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">        x = torch.nn.functional.pad(x, pad, mode=<span class="string">"constant"</span>, value=<span class="number">0</span>) <span class="comment"># x.shape={Tensor: (8, 128, 257, 257)}</span></span><br><span class="line">        x = self.conv(x) <span class="comment"># x.shape={Tensor: (8, 128, 128, 128)}</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        x = torch.nn.functional.avg_pool2d(x, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h3 id="残差块的-forward"><a href="#残差块的-forward" class="headerlink" title="残差块的 forward"></a>残差块的 forward</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_forward</span>(<span class="params">self, x, temb</span>):</span><br><span class="line">	h = x</span><br><span class="line">	h = self.norm1(h) <span class="comment"># GroupNorm: 32组，输入128通道的话（每组4通道），仿射变换（γ/β×128(通道数)对），输出形状保持不变</span></span><br><span class="line">	h = nonlinearity(h) <span class="comment"># SiLU激活: 对每个元素应用x*sigmoid(x)函数，增加非线性，inplace=True直接在原内存上修改以节省空间. sigmoid(x) = 1 / (1 + e^(-x))</span></span><br><span class="line">	h = self.conv1(h) <span class="comment"># 3x3卷积 输出格式(8, out_channels, 256, 256)</span></span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> temb <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">		h = h + self.temb_proj(nonlinearity(temb))[:,:,<span class="literal">None</span>,<span class="literal">None</span>]</span><br><span class="line"></span><br><span class="line">	h = self.norm2(h)</span><br><span class="line">	h = nonlinearity(h)</span><br><span class="line">	h = self.dropout(h) <span class="comment"># Apply dropout to prevent overfitting - randomly zeros out elements with probability p during training</span></span><br><span class="line">	h = self.conv2(h)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> self.in_channels != self.out_channels: <span class="comment"># 调整x(原始信息)的通道数, 使得与h(卷积后的信息)的通道数一致</span></span><br><span class="line">		<span class="keyword">if</span> self.use_conv_shortcut:</span><br><span class="line">			x = self.conv_shortcut(x) <span class="comment"># 3x3卷积, 这种方式不仅调整通道数，还能提取额外的空间特征</span></span><br><span class="line">		<span class="keyword">else</span>:</span><br><span class="line">			x = self.nin_shortcut(x) <span class="comment"># 1x1卷积, 只调整通道数，不提取空间特征</span></span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> x+h</span><br></pre></td></tr></table></figure>
<h4 id="关于残差的解释"><a href="#关于残差的解释" class="headerlink" title="关于残差的解释"></a>关于残差的解释</h4><p>好的，我们来用初中生能听懂的话解释一下 ResNet。</p>
<p>想象一下，我们正在教一台电脑认识图片，比如区分猫和狗。我们知道，让电脑学习得更好，通常需要给它很多很多层（就像很多很多步骤）来处理图片的信息。每一层都学习图片的一些特征，比如第一层可能找边缘，第二层可能找形状，后面的层就组合这些形状来判断是猫还是狗。</p>
<p>所以，我们想把这个学习机器做得<strong>非常深</strong>，有很多很多层，希望它能学得更厉害。</p>
<p><strong>问题来了：</strong></p>
<p>当我们把这个机器做得<strong>太深</strong>的时候，奇怪的事情发生了：它反而学得<strong>更差</strong>了！</p>
<p>这就像玩“传话”游戏。如果只有几个人传话，信息不容易出错。但如果让几十个人、上百个人一个接一个地传话，到最后，最开始的信息可能已经完全变样了，甚至变得乱七八糟，根本听不懂了。</p>
<p>在深度学习里，信息经过太多层处理，也会出现类似的问题：最开始学到的重要信息，在经过后面很多层的时候，可能会被“稀释”或者“忘记”，导致整个学习过程变得困难，甚至学不好。</p>
<p><strong>ResNet 是怎么解决这个问题的呢？</strong></p>
<p>ResNet 的想法非常聪明，它加了一个“<strong>抄近路</strong>”或者叫“<strong>跳跃连接</strong>”。</p>
<p>想象一下，信息从一层传到下一层，就像走一条路。ResNet 在这条路上加了一条“<strong>捷径</strong>”。</p>
<p>具体来说，当信息经过几层处理后，ResNet 不仅仅把处理后的结果往下传，它还会把<strong>原始的、没有经过这几层处理的信息</strong>也复制一份，然后把这份<strong>原始信息</strong>和<strong>经过处理后的信息**</strong>加在一起**，再往下传。</p>
<p><strong>这有什么好处呢？</strong></p>
<ol>
<li><strong>防止信息丢失：</strong> 就像你在传话的时候，如果担心信息传丢，你可以时不时地把最开始的话再重复一遍给后面的人听。ResNet 的“抄近路”就是这个作用，它保证了原始的重要信息能够直接传递到后面的层，不会在中间的层里完全丢失或变质。</li>
<li><strong>让学习更容易：</strong> 想象一下，如果让你学习一个非常复杂的知识，你可以选择从头开始学每一个细节，也可以选择在学习新知识的同时，时不时地回顾和利用你已经掌握的基础知识。ResNet 的“抄近路”让网络更容易学习“<strong>变化</strong>”或者“<strong>残差</strong>”（这就是 ResNet 名字里“Res”的意思），而不是每次都必须从零开始学习全新的东西。这让深层网络更容易训练。</li>
</ol>
<p><strong>简单来说：</strong></p>
<p>ResNet 就像给深度学习网络加了“<strong>记忆通道</strong>”或者“<strong>信息备份线</strong>”。它确保了即使网络变得非常深，最开始学到的重要信息也能通过“抄近路”的方式直接传递到后面的层，从而解决了深层网络学习困难的问题，让我们可以构建更深、更强大的网络来完成更复杂的任务，比如更准确地识别图片。</p>
<p>所以，ResNet 的核心思想就是那个“<strong>抄近路</strong>”或者叫“<strong>跳跃连接</strong>”，它把原始信息和处理后的信息加在一起，帮助深层网络更好地学习。</p>
<h3 id="降采样"><a href="#降采样" class="headerlink" title="降采样"></a>降采样</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> i_level != self.num_resolutions-<span class="number">1</span>:</span><br><span class="line">    hs.append(self.down[i_level].downsample(hs[-<span class="number">1</span>]))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Downsample</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, with_conv</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.with_conv = with_conv</span><br><span class="line">        <span class="keyword">if</span> self.with_conv:</span><br><span class="line">            <span class="comment"># no asymmetric padding in torch conv, must do it ourselves</span></span><br><span class="line">            self.conv = torch.nn.Conv2d(in_channels,</span><br><span class="line">                                        in_channels,</span><br><span class="line">                                        kernel_size=<span class="number">3</span>,</span><br><span class="line">                                        stride=<span class="number">2</span>,</span><br><span class="line">                                        padding=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">if</span> self.with_conv:</span><br><span class="line">            pad = (<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">            x = torch.nn.functional.pad(x, pad, mode=<span class="string">"constant"</span>, value=<span class="number">0</span>)</span><br><span class="line">            x = self.conv(x)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            x = torch.nn.functional.avg_pool2d(x, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<ul>
<li><p>卷积下采样 （ with_conv=True ）：</p>
<ul>
<li>首先应用非对称填充： pad = (0,1,0,1) ，这表示在右侧和底部各填充1个像素</li>
<li>填充模式为”constant”，填充值为常数值0</li>
<li>然后应用之前定义的卷积层，步长为2，实现下采样</li>
<li>注释中提到”no asymmetric padding in torch conv”，这是因为PyTorch的卷积层只支持对称填充，所以需要手动实现非对称填充</li>
</ul>
</li>
<li><p>平均池化下采样 （ with_conv=False ）：</p>
<ul>
<li>使用2×2的平均池化，步长为2</li>
<li>这种方式计算量更小，但可能会丢失更多信息</li>
</ul>
</li>
</ul>
<h3 id="middle-amp-amp-end"><a href="#middle-amp-amp-end" class="headerlink" title="middle && end"></a>middle &amp;&amp; end</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># middle</span></span><br><span class="line">self.mid = nn.Module()</span><br><span class="line">self.mid.block_1 = ResnetBlock(in_channels=block_in,</span><br><span class="line">							   out_channels=block_in,</span><br><span class="line">							   temb_channels=self.temb_ch,</span><br><span class="line">							   dropout=dropout) <span class="comment"># 和上面残差块相同</span></span><br><span class="line">self.mid.attn_1 = AttnBlock(block_in)</span><br><span class="line">self.mid.block_2 = ResnetBlock(in_channels=block_in,</span><br><span class="line">							   out_channels=block_in,</span><br><span class="line">							   temb_channels=self.temb_ch,</span><br><span class="line">							   dropout=dropout)</span><br><span class="line"></span><br><span class="line"><span class="comment"># end</span></span><br><span class="line">self.norm_out = Normalize(block_in)</span><br><span class="line">self.conv_out = torch.nn.Conv2d(block_in,</span><br><span class="line">								<span class="number">2</span>*z_channels <span class="keyword">if</span> double_z <span class="keyword">else</span> z_channels,</span><br><span class="line">								kernel_size=<span class="number">3</span>,</span><br><span class="line">								stride=<span class="number">1</span>,</span><br><span class="line">								padding=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># middle</span></span><br><span class="line">h = hs[-<span class="number">1</span>]  <span class="comment"># 获取最后的特征图</span></span><br><span class="line">h = self.mid.block_1(h, temb)  <span class="comment"># 第一个中间 ResNet 块</span></span><br><span class="line">h = self.mid.attn_1(h)         <span class="comment"># 中间注意力层</span></span><br><span class="line">h = self.mid.block_2(h, temb)  <span class="comment"># 第二个中间 ResNet 块</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># end</span></span><br><span class="line">h = self.norm_out(h)     <span class="comment"># 标准化， 组归一化(和残差块中的norm一样)</span></span><br><span class="line">h = nonlinearity(h)      <span class="comment"># 非线性激活</span></span><br><span class="line">h = self.conv_out(h)     <span class="comment"># 最终卷积层</span></span><br><span class="line"><span class="keyword">return</span> h</span><br></pre></td></tr></table></figure>
<p><code>middle</code> 部分位于下采样(downSampling)阶段之后，是编码器的中间处理模块。它的主要功能是：</p>
<ol>
<li>深度特征处理 ：在所有下采样完成后，对最小分辨率的特征图进行进一步处理</li>
<li>全局信息整合 ：通过注意力机制捕获特征图中的长距离依赖关系</li>
</ol>
<p><code>end</code>部分是编码器的最后阶段，负责将处理后的特征映射到潜在空间。它的主要功能是：</p>
<ol>
<li>特征标准化 ：确保特征分布稳定</li>
<li>非线性变换 ：增加模型表达能力</li>
<li>通道映射 ：将特征映射到潜在空间的维度</li>
</ol>
<h2 id="VQEmbedding"><a href="#VQEmbedding" class="headerlink" title="VQEmbedding"></a>VQEmbedding</h2><p><code>VQEmbedding</code> 类是 VQ-VAE (Vector Quantized Variational Autoencoder) 或 RQ-VAE (Residual Quantization VAE) 中的核心组件，负责执行向量量化操作。它的主要功能是将输入的连续特征向量映射到离散的码本（codebook）中的一个码字（embedding vector），并提供一种机制来更新这个码本。</p>
<p>这个类继承自 PyTorch 的 <code>nn.Embedding</code> 模块，并在此基础上增加了 EMA (Exponential Moving Average) 更新码本和处理未使用码字的功能。</p>
<h3 id="核心思想："><a href="#核心思想：" class="headerlink" title="核心思想："></a><strong>核心思想：</strong></h3><ol>
<li><strong>码本作为 <code>nn.Embedding.weight</code>：</strong> 利用 <code>nn.Embedding</code> 的 <code>weight</code> 参数来存储码本中的所有码字。<code>weight</code> 是一个形状为 <code>(num_embeddings, embedding_dim)</code> 的张量，其中 <code>num_embeddings</code> 是码本大小，<code>embedding_dim</code> 是码字的维度。</li>
<li><strong>查找最近邻：</strong> 对于输入的连续特征向量，计算它与码本中所有码字的距离，并找到距离最近的那个码字对应的索引。</li>
<li><strong>码字替换：</strong> 使用找到的索引从码本中查找对应的码字向量作为量化结果。</li>
<li><strong>码本更新 (EMA)：</strong> 在训练过程中，使用 EMA 机制根据当前批次中特征向量到码字的分配情况来更新码本中的码字。这是一种替代标准梯度下降更新码本的方法。</li>
<li><strong>处理未使用码字：</strong> 识别在 EMA 更新中长时间未被使用的码字，并用新的向量替换它们，以提高码本的利用率。</li>
</ol>
<h3 id="类结构和方法解析："><a href="#类结构和方法解析：" class="headerlink" title="类结构和方法解析："></a><strong>类结构和方法解析：</strong></h3><ol>
<li><p><strong><code>__init__(self, n_embed, embed_dim, ema=True, decay=0.99, restart_unused_codes=True, eps=1e-5)</code></strong></p>
<ul>
<li><strong>目的：</strong> 初始化 VQ 嵌入模块。</li>
<li><strong>参数：</strong><ul>
<li><code>n_embed</code>: 码本中实际码字的数量。</li>
<li><code>embed_dim</code>: 每个码字的维度。</li>
<li><code>ema</code>: 是否使用 EMA 更新码本。如果为 <code>False</code>，则码本不会被更新（或者需要外部机制更新）。</li>
<li><code>decay</code>: EMA 的衰减率，用于平滑更新。值越接近 1，EMA 越稳定。</li>
<li><code>restart_unused_codes</code>: 是否启用未使用码字的重启机制。</li>
<li><code>eps</code>: 用于数值稳定的小值，特别是在 EMA 更新中除以使用计数时。</li>
</ul>
</li>
<li><strong>初始化过程：</strong><ul>
<li><code>super().__init__(n_embed + 1, embed_dim, padding_idx=n_embed)</code>: 调用父类 <code>nn.Embedding</code> 的构造函数。注意这里码本大小是 <code>n_embed + 1</code>。最后一个索引 <code>n_embed</code> 被指定为 <code>padding_idx</code>。这意味着 <code>self.weight</code> 的形状是 <code>(n_embed + 1, embed_dim)</code>。实际用于量化的码字是索引 <code>0</code> 到 <code>n_embed - 1</code> 对应的向量。索引 <code>n_embed</code> 对应的向量是 padding 向量，通常不参与量化或 EMA 更新。</li>
<li>存储参数 <code>ema</code>, <code>decay</code>, <code>eps</code>, <code>restart_unused_codes</code>, <code>n_embed</code>。</li>
<li><code>if self.ema:</code>: 如果启用 EMA：<ul>
<li><code>_ = [p.requires_grad_(False) for p in self.parameters()]</code>: <strong>关键一步！</strong> 禁用所有参数（包括 <code>self.weight</code>）的梯度计算。这意味着码本 <code>self.weight</code> 不会通过标准的 <code>loss.backward()</code> 来更新，而是完全依赖于 EMA 机制。</li>
<li><code>self.register_buffer('cluster_size_ema', torch.zeros(n_embed))</code>: 注册一个 buffer 来存储每个码字（索引 0 到 <code>n_embed - 1</code>）的 EMA 使用计数。初始化为零。Buffer 是模型状态的一部分，但不是需要梯度的参数。</li>
<li><code>self.register_buffer('embed_ema', self.weight[:-1, :].detach().clone())</code>: 注册一个 buffer 来存储每个码字（索引 0 到 <code>n_embed - 1</code>）的 EMA 累积向量和。初始化为码本的初始值。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong><code>compute_distances(self, inputs)</code></strong> (<code>@torch.no_grad()</code>)</p>
<ul>
<li><strong>目的：</strong> 计算输入向量与码本中所有码字之间的平方欧几里得距离。</li>
<li><strong>输入：</strong> <code>inputs</code> (形状 <code>..., embed_dim</code>)，输入的连续特征向量。</li>
<li><strong>过程：</strong><ul>
<li>获取实际码本（排除 padding 向量）：<code>self.weight[:-1, :]</code>。</li>
<li>使用高效的矩阵乘法计算平方欧几里得距离：<code>||a - b||^2 = ||a||^2 + ||b||^2 - 2a^T b</code>。</li>
<li>将结果重塑回原始输入形状加上码本大小的维度。</li>
</ul>
</li>
<li><strong>输出：</strong> <code>distances</code> (形状 <code>..., n_embed</code>)，其中 <code>distances[..., i]</code> 是输入向量与第 <code>i</code> 个码字的平方距离。<code>@torch.no_grad()</code> 确保此操作不记录梯度。</li>
</ul>
</li>
<li><p><strong><code>find_nearest_embedding(self, inputs)</code></strong> (<code>@torch.no_grad()</code>)</p>
<ul>
<li><strong>目的：</strong> 找到输入向量在码本中的最近邻码字对应的索引。</li>
<li><strong>输入：</strong> <code>inputs</code> (形状 <code>..., embed_dim</code>)。</li>
<li><strong>过程：</strong><ul>
<li>调用 <code>compute_distances</code> 计算距离。</li>
<li><code>distances.argmin(dim=-1)</code>: 在最后一个维度（码本维度）上找到最小值对应的索引。</li>
</ul>
</li>
<li><strong>输出：</strong> <code>embed_idxs</code> (形状 <code>...</code>)，最近邻码字的索引。<code>@torch.no_grad()</code> 确保此操作不记录梯度。这是量化过程中的离散步骤。</li>
</ul>
</li>
<li><p><strong><code>_tile_with_noise(self, x, target_n)</code></strong> (<code>@torch.no_grad()</code>)</p>
<ul>
<li><strong>目的：</strong> 辅助 <code>restart_unused_codes</code> 功能，从输入向量中生成一个带有噪声的、数量足够的向量池。</li>
<li><strong>输入：</strong> <code>x</code> (形状 <code>B, embed_dim</code>)，当前批次的输入向量；<code>target_n</code> (整数)，目标向量数量。</li>
<li><strong>过程：</strong> 将输入批次重复多次直到数量达到 <code>target_n</code>，并添加少量高斯噪声。</li>
<li><strong>输出：</strong> 形状为 <code>(&gt;=target_n, embed_dim)</code> 的向量张量。<code>@torch.no_grad()</code> 确保此操作不记录梯度。</li>
</ul>
</li>
<li><p><strong><code>_update_buffers(self, vectors, idxs)</code></strong> (<code>@torch.no_grad()</code>)</p>
<ul>
<li><strong>目的：</strong> 根据当前批次的输入向量及其对应的最近邻索引，更新 EMA 缓冲区 (<code>cluster_size_ema</code> 和 <code>embed_ema</code>)。</li>
<li><strong>输入：</strong> <code>vectors</code> (形状 <code>..., embed_dim</code>)，当前批次的输入向量；<code>idxs</code> (形状 <code>...</code>)，这些向量对应的最近邻码字索引。</li>
<li><strong>过程：</strong><ul>
<li>将输入向量和索引展平。</li>
<li>计算当前批次中每个码字的使用次数 (<code>cluster_size</code>) 和分配给每个码字的向量之和 (<code>vectors_sum_per_cluster</code>)。这通过构建一个 one-hot 矩阵并进行矩阵乘法高效完成。</li>
<li><code>if dist.is_initialized(): dist.all_reduce(...)</code>: 如果使用分布式训练，将这些统计量在所有进程之间进行累加，确保 EMA 更新基于全局数据。</li>
<li><strong>EMA 更新：</strong> 使用 <code>self.decay</code> 更新 <code>self.cluster_size_ema</code> 和 <code>self.embed_ema</code>。</li>
<li><code>if self.restart_unused_codes:</code>: 如果启用未使用码字重启：<ul>
<li>使用 <code>_tile_with_noise</code> 从当前批次生成一个随机向量池。</li>
<li><code>if dist.is_initialized(): dist.broadcast(...)</code>: 在分布式训练中，确保所有进程使用相同的随机向量池（通常从主进程广播）。</li>
<li>识别 EMA 使用计数小于 1 的码字（即未使用码字）。</li>
<li>用随机向量池中的向量替换这些未使用码字的 <code>embed_ema</code>。</li>
<li>将这些被替换码字的 <code>cluster_size_ema</code> 重置为 1。</li>
</ul>
</li>
</ul>
</li>
<li><strong>输出：</strong> 无。<code>@torch.no_grad()</code> 确保此操作不记录梯度。</li>
</ul>
</li>
<li><p><strong><code>_update_embedding(self)</code></strong> (<code>@torch.no_grad()</code>)</p>
<ul>
<li><strong>目的：</strong> 使用 EMA 缓冲区中的统计量来更新实际的码本权重 <code>self.weight</code>。</li>
<li><strong>输入：</strong> 无。</li>
<li><strong>过程：</strong><ul>
<li>计算归一化的使用计数：<code>normalized_cluster_size = (n * (self.cluster_size_ema + self.eps) / (n + n_embed * self.eps))</code>。这里 <code>n</code> 是总的使用计数。添加 <code>self.eps</code> 防止除以零，并进行平滑。</li>
<li>更新码本权重：<code>self.weight[:-1, :] = self.embed_ema / normalized_cluster_size.reshape(-1, 1)</code>。新的码字是 EMA 累积向量和除以归一化的 EMA 使用计数。</li>
</ul>
</li>
<li><strong>输出：</strong> 无。<code>@torch.no_grad()</code> 确保此操作不记录梯度。</li>
</ul>
</li>
<li><p><strong><code>forward(self, inputs)</code></strong></p>
<ul>
<li><strong>目的：</strong> 执行 VQ 量化的前向传播。</li>
<li><strong>输入：</strong> <code>inputs</code> (形状 <code>..., embed_dim</code>)。</li>
<li><strong>过程：</strong><ul>
<li><code>embed_idxs = self.find_nearest_embedding(inputs)</code>: 找到最近邻码字的索引。</li>
<li><code>if self.training and self.ema:</code>: 如果在训练模式且启用 EMA：<ul>
<li><code>self._update_buffers(inputs, embed_idxs)</code>: 根据当前批次更新 EMA 缓冲区。</li>
</ul>
</li>
<li><code>embeds = self.embed(embed_idxs)</code>: 使用找到的索引从<strong>当前的</strong> <code>self.weight</code> 中查找对应的码字向量。</li>
<li><code>if self.ema and self.training:</code>: 如果在训练模式且启用 EMA：<ul>
<li><code>self._update_embedding()</code>: 使用更新后的 EMA 缓冲区来更新 <code>self.weight</code>。<strong>注意：</strong> 这个更新发生在查找之后，意味着当前批次的查找使用了更新前的码本，但码本在当前前向传播结束时被更新，供下一个批次使用。</li>
</ul>
</li>
</ul>
</li>
<li><strong>输出：</strong> <code>embeds</code> (形状 <code>..., embed_dim</code>)，量化后的向量（即查找到的码字）；<code>embed_idxs</code> (形状 <code>...</code>)，选定的码字索引。</li>
</ul>
</li>
<li><p><strong><code>embed(self, idxs)</code></strong></p>
<ul>
<li><strong>目的：</strong> 标准的嵌入查找方法，用于根据索引获取码字向量。</li>
<li><strong>输入：</strong> <code>idxs</code> (形状 <code>...</code>)，码字索引。</li>
<li><strong>过程：</strong> 调用父类 <code>nn.Embedding</code> 的 <code>forward</code> 方法进行查找。</li>
<li><strong>输出：</strong> 形状为 <code>idxs.shape + (embed_dim,)</code> 的码字向量。</li>
</ul>
</li>
</ol>
<h3 id="总结："><a href="#总结：" class="headerlink" title="总结："></a><strong>总结：</strong></h3><p><code>VQEmbedding</code> 类是一个带有 EMA 更新和未使用码字重启功能的向量量化模块。它利用 <code>nn.Embedding</code> 的 <code>weight</code> 作为码本，通过计算距离找到最近邻码字，并使用 EMA 机制（而不是梯度下降）来更新码本。<code>@torch.no_grad()</code> 装饰器被广泛用于量化查找和码本更新步骤，因为这些步骤不应参与标准的反向传播。EMA 更新缓冲区 (<code>cluster_size_ema</code>, <code>embed_ema</code>) 记录了码字的使用情况和累积向量和，并周期性地用于更新实际的码本权重 <code>self.weight</code>。<code>restart_unused_codes</code> 机制进一步提高了码本的效率。</p>
<h2 id="quantizer"><a href="#quantizer" class="headerlink" title="quantizer"></a>quantizer</h2><h2 id="损失"><a href="#损失" class="headerlink" title="损失"></a>损失</h2><h3 id="commit损失"><a href="#commit损失" class="headerlink" title="commit损失"></a>commit损失</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">commitment_loss = self.compute_commitment_loss(x_reshaped, quant_list)</span><br></pre></td></tr></table></figure>
<p>在计算 <code>commitment_loss</code> 时：</p>
<ul>
<li><strong><code>x_reshaped</code></strong>: 原始图像经过编码器（Encoder）处理后得到的潜在特征（latent features），然后通过 to_code_shape 方法进行重新整形，这一步将形状为 (B, H, W, D) 的特征张量重组为形状为 (B, h, w, embed_dim) 的张量。具体来说，它将空间维度 H 和 W 分别划分为 h×rH 和 w×rW ，然后重新排列这些维度，将每个空间位置的特征维度扩展，从而降低空间分辨率但增加特征维度，为残差量化过程做准备。</li>
<li><strong><code>quant_list</code></strong>: 是一个<strong>列表</strong>，包含残差量化过程中<strong>每一层累积</strong>得到的量化特征。列表中的每个元素形状也是 <code>(B, h, w, embed_dim)</code>。列表长度取决于量化层数。</li>
</ul>
<p><code>commitment_loss</code> 衡量的是 <code>x_reshaped</code> 与 <code>quant_list</code> 中<strong>每一层累积量化结果</strong>之间的差异。计算输入特征 x 与各个码本累积量化结果之间的均方误差（MSE）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_commitment_loss</span>(<span class="params">self, x, quant_list</span>):</span><br><span class="line">	<span class="string">r"""</span></span><br><span class="line"><span class="string">	Compute the commitment loss for the residual quantization.</span></span><br><span class="line"><span class="string">	The loss is iteratively computed by aggregating quantized features.</span></span><br><span class="line"><span class="string">	"""</span></span><br><span class="line">	loss_list = []</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">for</span> idx, quant <span class="keyword">in</span> <span class="built_in">enumerate</span>(quant_list):</span><br><span class="line">		partial_loss = (x-quant.detach()).<span class="built_in">pow</span>(<span class="number">2.0</span>).mean()</span><br><span class="line">		loss_list.append(partial_loss)</span><br><span class="line">	</span><br><span class="line">	commitment_loss = torch.mean(torch.stack(loss_list))</span><br><span class="line">	<span class="keyword">return</span> commitment_loss</span><br></pre></td></tr></table></figure>
<h2 id="编码器和解码器结构设计不同的原因"><a href="#编码器和解码器结构设计不同的原因" class="headerlink" title="编码器和解码器结构设计不同的原因"></a>编码器和解码器结构设计不同的原因</h2><h3 id="编码器结构（先降采样，再middle，最后end）"><a href="#编码器结构（先降采样，再middle，最后end）" class="headerlink" title="编码器结构（先降采样，再middle，最后end）"></a>编码器结构（先降采样，再middle，最后end）</h3><p>编码器的结构设计遵循了”逐步压缩信息”的原则：</p>
<ol>
<li><p><strong>先降采样</strong>：</p>
<ul>
<li>目的是逐步减小特征图的空间尺寸，同时增加通道数</li>
<li>通过多个下采样层，将高分辨率的输入图像（如256×256）压缩到低分辨率（如8×8）</li>
<li>每次降采样都伴随着特征通道数的增加，保留更多抽象特征</li>
</ul>
</li>
<li><p><strong>中间层（middle）</strong>：</p>
<ul>
<li>在最低分辨率上处理高维特征</li>
<li>使用ResNet块和注意力机制捕获全局依赖关系</li>
<li>在压缩的特征空间中进一步提取和重组信息</li>
</ul>
</li>
<li><p><strong>结束层（end）</strong>：</p>
<ul>
<li>将高维特征映射到适合量化的低维空间</li>
<li>准备特征用于后续的向量量化过程</li>
</ul>
</li>
</ol>
<h3 id="解码器结构（先middle，再上采样，最后end）"><a href="#解码器结构（先middle，再上采样，最后end）" class="headerlink" title="解码器结构（先middle，再上采样，最后end）"></a>解码器结构（先middle，再上采样，最后end）</h3><p>解码器的结构设计遵循了”逐步重建信息”的原则：</p>
<ol>
<li><p><strong>先middle层</strong>：</p>
<ul>
<li>接收量化后的低分辨率特征</li>
<li>通过ResNet块和注意力机制重新处理这些压缩特征</li>
<li>为后续的上采样过程准备丰富的特征表示</li>
</ul>
</li>
<li><p><strong>上采样层</strong>：</p>
<ul>
<li>逐步增加特征图的空间尺寸，同时减少通道数</li>
<li>从低分辨率（如8×8）逐步恢复到原始分辨率（如256×256）</li>
<li>每次上采样都伴随着特征通道数的减少，逐步恢复空间细节</li>
</ul>
</li>
<li><p><strong>结束层（end）</strong>：</p>
<ul>
<li>将特征映射回原始图像空间（通常是RGB三通道）</li>
<li>生成最终的重建图像</li>
</ul>
</li>
</ol>
<h3 id="为什么采用这种对称但不完全相同的结构？"><a href="#为什么采用这种对称但不完全相同的结构？" class="headerlink" title="为什么采用这种对称但不完全相同的结构？"></a>为什么采用这种对称但不完全相同的结构？</h3><ol>
<li><p><strong>信息流向的考虑</strong>：</p>
<ul>
<li>编码器：从高维空间（图像）→低维空间（潜在表示）</li>
<li>解码器：从低维空间（潜在表示）→高维空间（图像）</li>
</ul>
</li>
<li><p><strong>处理顺序的逻辑性</strong>：</p>
<ul>
<li>编码器需要先降低分辨率再进行全局处理，因为这样可以在计算效率高的低分辨率下捕获全局特征</li>
<li>解码器需要先在低分辨率下处理全局信息，然后再逐步恢复空间细节</li>
</ul>
</li>
<li><p><strong>计算效率</strong>：</p>
<ul>
<li>middle层通常包含注意力机制，在低分辨率下计算注意力更加高效</li>
<li>在编码器降采样后和解码器上采样前应用middle层，可以在最小的特征图尺寸上处理全局依赖关系</li>
</ul>
</li>
<li><p><strong>特征转换的自然流程</strong>：</p>
<ul>
<li>编码过程：细节压缩→抽象表示→潜在编码</li>
<li>解码过程：潜在编码→特征扩展→细节重建</li>
</ul>
</li>
</ol>
<p>总的来说，这种设计反映了自编码器的基本原理：编码器负责将高维输入压缩到低维潜在空间，解码器负责从低维潜在空间重建高维输出。两者的结构虽然在某种程度上是对称的（都包含降采样/上采样和middle层），但处理顺序的不同反映了它们在信息流动方向上的根本差异。</p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2025/02/27/JS%E9%80%86%E5%90%91/" rel="prev" title="JS逆向">
      <i class="fa fa-chevron-left"></i> JS逆向
    </a></div>
      <div class="post-nav-item">
    <a href="/2025/03/13/keymaps/" rel="next" title="keymaps">
      keymaps <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Residual-Quantization"><span class="nav-number">1.</span> <span class="nav-text">Residual Quantization</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3"><span class="nav-number">1.0.0.1.</span> <span class="nav-text">核心思想</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%B7%E4%BD%93%E6%B5%81%E7%A8%8B"><span class="nav-number">1.0.0.2.</span> <span class="nav-text">具体流程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%B1%E4%BA%AB%E4%BB%A3%E7%A0%81%E6%9C%AC%E7%9A%84%E4%BC%98%E5%8A%BF"><span class="nav-number">1.0.0.3.</span> <span class="nav-text">共享代码本的优势</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%8EVQ%E7%9A%84%E5%AF%B9%E6%AF%94"><span class="nav-number">1.0.0.4.</span> <span class="nav-text">与VQ的对比</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%B3%E9%94%AE%E4%BC%98%E5%8A%BF"><span class="nav-number">1.0.0.5.</span> <span class="nav-text">关键优势</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BA%94%E7%94%A8%E6%84%8F%E4%B9%89"><span class="nav-number">1.0.0.6.</span> <span class="nav-text">应用意义</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#RQ-VAE%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%95"><span class="nav-number">2.</span> <span class="nav-text">RQ_VAE代码记录</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#outputs-model-xs"><span class="nav-number">2.1.</span> <span class="nav-text">outputs &#x3D; model(xs)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA"><span class="nav-number">2.1.1.</span> <span class="nav-text">输入输出</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Encoder"><span class="nav-number">2.2.</span> <span class="nav-text">Encoder</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA-1"><span class="nav-number">2.2.1.</span> <span class="nav-text">输入输出</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%AE%8B%E5%B7%AE%E5%9D%97"><span class="nav-number">2.2.2.</span> <span class="nav-text">残差块</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%B8%80%E6%AC%A1%E9%80%9A%E8%BF%87%E6%AE%8B%E5%B7%AE%E5%9D%97"><span class="nav-number">2.2.3.</span> <span class="nav-text">第一次通过残差块</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%AE%8B%E5%B7%AE%E5%9D%97%E7%9A%84-forward"><span class="nav-number">2.2.4.</span> <span class="nav-text">残差块的 forward</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%B3%E4%BA%8E%E6%AE%8B%E5%B7%AE%E7%9A%84%E8%A7%A3%E9%87%8A"><span class="nav-number">2.2.4.1.</span> <span class="nav-text">关于残差的解释</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%99%8D%E9%87%87%E6%A0%B7"><span class="nav-number">2.2.5.</span> <span class="nav-text">降采样</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#middle-amp-amp-end"><span class="nav-number">2.2.6.</span> <span class="nav-text">middle &amp;&amp; end</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#VQEmbedding"><span class="nav-number">2.3.</span> <span class="nav-text">VQEmbedding</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3%EF%BC%9A"><span class="nav-number">2.3.1.</span> <span class="nav-text">核心思想：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%B1%BB%E7%BB%93%E6%9E%84%E5%92%8C%E6%96%B9%E6%B3%95%E8%A7%A3%E6%9E%90%EF%BC%9A"><span class="nav-number">2.3.2.</span> <span class="nav-text">类结构和方法解析：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%BB%E7%BB%93%EF%BC%9A"><span class="nav-number">2.3.3.</span> <span class="nav-text">总结：</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#quantizer"><span class="nav-number">2.4.</span> <span class="nav-text">quantizer</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8D%9F%E5%A4%B1"><span class="nav-number">2.5.</span> <span class="nav-text">损失</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#commit%E6%8D%9F%E5%A4%B1"><span class="nav-number">2.5.1.</span> <span class="nav-text">commit损失</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BC%96%E7%A0%81%E5%99%A8%E5%92%8C%E8%A7%A3%E7%A0%81%E5%99%A8%E7%BB%93%E6%9E%84%E8%AE%BE%E8%AE%A1%E4%B8%8D%E5%90%8C%E7%9A%84%E5%8E%9F%E5%9B%A0"><span class="nav-number">2.6.</span> <span class="nav-text">编码器和解码器结构设计不同的原因</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BC%96%E7%A0%81%E5%99%A8%E7%BB%93%E6%9E%84%EF%BC%88%E5%85%88%E9%99%8D%E9%87%87%E6%A0%B7%EF%BC%8C%E5%86%8Dmiddle%EF%BC%8C%E6%9C%80%E5%90%8Eend%EF%BC%89"><span class="nav-number">2.6.1.</span> <span class="nav-text">编码器结构（先降采样，再middle，最后end）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A7%A3%E7%A0%81%E5%99%A8%E7%BB%93%E6%9E%84%EF%BC%88%E5%85%88middle%EF%BC%8C%E5%86%8D%E4%B8%8A%E9%87%87%E6%A0%B7%EF%BC%8C%E6%9C%80%E5%90%8Eend%EF%BC%89"><span class="nav-number">2.6.2.</span> <span class="nav-text">解码器结构（先middle，再上采样，最后end）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%87%87%E7%94%A8%E8%BF%99%E7%A7%8D%E5%AF%B9%E7%A7%B0%E4%BD%86%E4%B8%8D%E5%AE%8C%E5%85%A8%E7%9B%B8%E5%90%8C%E7%9A%84%E7%BB%93%E6%9E%84%EF%BC%9F"><span class="nav-number">2.6.3.</span> <span class="nav-text">为什么采用这种对称但不完全相同的结构？</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="QY"
      src="https://bigshuimu.oss-cn-nanjing.aliyuncs.com/personal/cat.jpg">
  <p class="site-author-name" itemprop="name">QY</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">6</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/bigshuimu" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;bigshuimu" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://armke.github.io/" title="https:&#x2F;&#x2F;armke.github.io&#x2F;" rel="noopener" target="_blank">armke</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">QY</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
